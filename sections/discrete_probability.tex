\section{Discrete Probability}
\label{sec:discrete-probability}

\subsection*{Random Variables}
\label{sec:probability}

A random variable is a function from the sample space $\Omega$ to the real numbers $\mathbb{R}$.

A random variable $X$ is discrete if it takes on a finite or countable number of values.

\subsubsection*{Probability Mass Function}

The probability mass function (PMF) or distribution of a discrete random variable $X$ gives the probabilities of its possible values.

$$ P(X = x) \geq 0 $$ (all probabilities are non-negative)
$$ \sum_{i} P(X = x_i) = 1 $$ (the probabilities sum to 1)

\subsubsection*{Cumulative Distribution Function}

The cumulative distribution function (CDF) of a discrete random variable $X$ gives the probability that $X$ is less than or equal to $x$.

$$ F: \mathbb{R} \rightarrow [0, 1] $$
$$ F(x) = P(X \leq x) $$

Similar to PDF graph except it adds them up (cumulative)

\begin{align*}
       & P(a < x \leq b)             \\
       & = P(X \leq b) - P(X \leq a) \\
       & = F(b) - F(a)
\end{align*}

A cumulative distribution function $F$:
\begin{itemize}
      \item is non-decreasing: $F(x) \leq F(y)$ for all $x \leq y$
      \item has limit 0: $F(-\infty) = 0$ on the left
      \item has limit 1: $F(\infty) = 1$ on the right
\end{itemize}

\subsubsection*{Expected Value}

The expected value of a discrete random variable $X$ is the average value of $X$.

$$ E(X) = \sum_{i} x_i P(X = x_i) $$ (provided the sum exists)

Note: Expected value need not be a possible value of $X$.

For $g: \mathbb{R} \rightarrow \mathbb{R}$:

\begin{align*}
       & E(g(X))                      \\
       & = \sum_{i} g(x_i) P(X = x_i)
\end{align*}

(provided the sum exists)

Expectation is linear, so $E(aX + b) = aE(X) + b$ for any constants $a$ and $b$.

\subsubsection*{Variance}

The variance tells us how surprised we should be if we observe a value of $X$.

$$ \text{Var}(X) = E(X^2) - (E(X))^2 $$

\subsubsection*{Standard Deviation}

The standard deviation is the square root of the variance.

$$ \text{SD}(X) = \sqrt{\text{Var}(X)} $$

\subsection*{Bernoulli and Binomial Distributions}

$$X \sim \text{Binom}(n,p) $$

$X$ has the Binomial distribution with parameters $n$ and $p$ if, for $n$ independent trials, each succeeding with probability $p$, the random variable $X$ counts the number of successes within the $n$ trials.

Special case $n=1$ is called the Bernoulli distribution with parameter $p$. In this case, $X$ is 1 if the trial succeeds and 0 if it fails (indication variable).

\subsubsection*{PMF}

Let $X \sim \text{Binom}(n,p)$ and $X = 0,1,\dots,n$. Then:

\begin{align*}
       & P(X = k)                       \\
       & = \binom{n}{k} p^k (1-p)^{n-k}
\end{align*}

The Bernouilli($p$) distribution can take on values $0$ or $1$ with properties:

$$ P(X = 0) = 1 - p $$
$$ P(X = 1) = p $$

\subsubsection*{Newton's Binomial Theorem:}

\begin{align*}
       & \sum_{k=0}^n \binom{n}{k} a^k b^{n-k} \\
       & =(a + b)^n
\end{align*}

\subsubsection*{CDF}

For $X\sim \text{Binom}(n,p)$, the CDF is:

\begin{align*}
      B(x;n,p) & = P(X \leq x)           \\
               & = \sum_{y=0}^x b(y;n,p)
\end{align*}

\subsubsection*{Expected Value}

$$ E(X) = np $$

\subsubsection*{Variance}

$$ \text{Var}(X) = np(1-p) $$

\subsection*{Poisson Distribution}

$$ X \sim \text{Poisson}(\lambda) $$

The random variable $X$ is Poisson distributed with parameter $\lambda$ if $\lambda$ is non-negative integer valued and its mass function is:

$$ P(X = i) = e^{-\lambda} \times \frac{\lambda^i}{i!} $$

\subsubsection*{Poisson Approximation to Binomial}

Take $Y \sim \text{Binom}(n,p)$ with large $n$ and small $p$, such that $np \approx \lambda$. Then $Y$ is  approximately Poisson($\lambda$) distributed.

\subsubsection*{Expected Value and Variance}

$$ E(X) = \text{Var}(X) = \lambda $$

... since Binomial expectation and variance are $np$ and $np(1-p)$ which both converge to $\lambda$ for large $n$.

\subsection*{Geometric Distribution}

When is the first success?

$$ X \sim \text{Geom}(p) $$

Suppose that independent trials, each succeeding with probability $p$, are repeated until the first success. The total number $X$ of trials made has the Geometric($p$) distribution.

$X$ can take on positive integers, with probabilities:

$$ P(X = i) = (1-p)^{i-1} p $$

The Geometric random variable is (discrete) memoryless:

\begin{align*}
       & P(X > n + k | X > n) \\
       & = P(X > k)
\end{align*}

... for every $k \geq 1$, $n \geq 0$.

\subsubsection*{Expectation and Variance}

$$ E(X) = \frac{1}{p} $$

$$ \text{Var}(X) = \frac{1-p}{p^2} $$